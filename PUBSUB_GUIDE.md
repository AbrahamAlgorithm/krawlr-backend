# Pub/Sub Queue System - Deployment Guide

## ğŸš€ High-Performance Async Architecture

Your application now uses **Google Cloud Pub/Sub** for massive speed and scalability:

- âœ… **Instant API responses** (< 1 second) - jobs queued immediately
- âœ… **Horizontal scaling** - run multiple workers in parallel
- âœ… **Guaranteed delivery** - jobs never lost, automatic retries
- âœ… **Real-time progress** - live updates via progress topic
- âœ… **Webhooks** - notify your systems when scrapes complete

## ğŸ“‹ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /api/v1/scrape/company
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI API   â”‚ â—„â”€â”€ Returns job_id instantly
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Publishes to Pub/Sub
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ scrape-jobs  â”‚ â—„â”€â”€ Pub/Sub Topic
   â”‚    Queue     â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼           â–¼         â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Worker 1â”‚  â”‚Worker 2â”‚ â”‚Worker 3â”‚ â”‚Worker Nâ”‚  â—„â”€â”€ Scale to N workers
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚           â”‚          â”‚          â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Firestore   â”‚ â—„â”€â”€ Job status + Results
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  scrape-completed      â”‚ â—„â”€â”€ Pub/Sub Topic
    â”‚       Topic            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Webhook Service â”‚ â—„â”€â”€ Sends HTTP webhooks
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Setup Instructions

### 1. Install Dependencies

```bash
pip install google-cloud-pubsub
```

### 2. Configure Environment Variables

Add to your `.env` file:

```bash
# Google Cloud Project
GCP_PROJECT_ID=your-project-id

# Pub/Sub Topics (optional - defaults provided)
PUBSUB_SCRAPE_JOBS_TOPIC=scrape-jobs
PUBSUB_SCRAPE_COMPLETED_TOPIC=scrape-completed
PUBSUB_SCRAPE_PROGRESS_TOPIC=scrape-progress
```

### 3. Setup Pub/Sub Infrastructure

Run once to create topics and subscriptions:

```bash
python setup_pubsub.py
```

### 4. Start Services

**Terminal 1 - API Server:**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 - Worker (Process scrape jobs):**
```bash
python worker.py
```

**Terminal 3 - Webhook Service (Send notifications):**
```bash
python webhook_service.py
```

## ğŸ“Š How It Works

### Submit Scrape Job (Client)

```bash
POST /api/v1/scrape/company
Authorization: Bearer YOUR_TOKEN

{
  "url": "https://stripe.com"
}

Response (< 1 second):
{
  "scrape_id": "abc-123-def",
  "status": "queued",
  "message": "Job queued successfully!"
}
```

### Check Status (Poll every 2-3 seconds)

```bash
GET /api/v1/scrape/status/abc-123-def

Response:
{
  "job_id": "abc-123-def",
  "status": "processing",
  "progress_percent": 50,
  "current_stage": "financial",
  "duration_seconds": 45
}
```

### Get Results (When complete)

```bash
GET /api/v1/scrape/abc-123-def

Response:
{
  "company": {...},
  "financials": {...},
  "funding": {...},
  ...
}
```

## ğŸ”¥ Production Deployment

### Docker Compose (Recommended)

```yaml
version: '3.8'

services:
  api:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json
    volumes:
      - ./credentials.json:/app/credentials.json
  
  worker:
    build: .
    command: python worker.py
    deploy:
      replicas: 3  # Scale workers
    environment:
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json
    volumes:
      - ./credentials.json:/app/credentials.json
  
  webhook:
    build: .
    command: python webhook_service.py
    environment:
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json
    volumes:
      - ./credentials.json:/app/credentials.json
```

Start with:
```bash
docker-compose up -d --scale worker=5
```

### Kubernetes (For massive scale)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scrape-worker
spec:
  replicas: 10  # Scale to 10+ workers
  selector:
    matchLabels:
      app: scrape-worker
  template:
    metadata:
      labels:
        app: scrape-worker
    spec:
      containers:
      - name: worker
        image: gcr.io/YOUR_PROJECT/krawlr-worker:latest
        command: ["python", "worker.py"]
        env:
        - name: GCP_PROJECT_ID
          value: "your-project-id"
        resources:
          requests:
            cpu: "1"
            memory: "2Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
```

## ğŸ“ˆ Performance Metrics

### Before Pub/Sub (FastAPI BackgroundTasks)
- â±ï¸ API response: 60-120 seconds (blocked)
- ğŸ”„ Concurrency: Limited by server workers
- ğŸ’¥ Failures: Jobs lost on server restart
- ğŸ“Š Scale: Vertical only (bigger server)

### After Pub/Sub
- âš¡ API response: < 1 second (queued)
- ğŸ”„ Concurrency: Unlimited (N workers)
- âœ… Failures: Automatic retry + durability
- ğŸ“Š Scale: Horizontal (add more workers)

### Example Throughput

**Single worker:**
- 90 seconds per scrape
- ~40 scrapes/hour
- ~960 scrapes/day

**10 workers:**
- 90 seconds per scrape (parallel)
- ~400 scrapes/hour
- ~9,600 scrapes/day

**100 workers:**
- ~4,000 scrapes/hour
- ~96,000 scrapes/day

## ğŸ”§ Monitoring

### Check Queue Depth

```bash
gcloud pubsub topics list
gcloud pubsub subscriptions list
```

### View Worker Logs

```bash
# Local
tail -f worker.log

# Docker
docker logs -f worker-1

# Kubernetes
kubectl logs -f deployment/scrape-worker
```

### Cloud Monitoring (GCP)

- Pub/Sub metrics: https://console.cloud.google.com/cloudpubsub
- Set alerts for:
  - Queue depth > 100 (scale up workers)
  - Old unacked messages (worker issues)
  - Failed delivery attempts (investigate errors)

## ğŸ¯ Auto-Scaling

### Cloud Run (Serverless Workers)

Deploy workers as Cloud Run service with:
- Min instances: 0 (cost-effective)
- Max instances: 100 (massive scale)
- Pub/Sub trigger: Auto-scales based on queue

```bash
gcloud run deploy scrape-worker \
  --image gcr.io/YOUR_PROJECT/worker:latest \
  --memory 2Gi \
  --timeout 600s \
  --min-instances 0 \
  --max-instances 100 \
  --set-env-vars GCP_PROJECT_ID=your-project
```

## ğŸ”’ Security

1. **Service Account**: Create dedicated service account with Pub/Sub permissions
2. **IAM Roles**:
   - `roles/pubsub.publisher` (API server)
   - `roles/pubsub.subscriber` (Workers, webhook service)
3. **Firestore Rules**: Ensure workers can read/write job data
4. **Webhook Signatures**: Add HMAC signatures to webhooks for verification

## ğŸ› Troubleshooting

### Jobs not processing
- Check worker is running: `ps aux | grep worker.py`
- Check Pub/Sub subscription exists
- Verify service account permissions

### Slow processing
- Scale up workers: `docker-compose up -d --scale worker=10`
- Check worker logs for errors
- Monitor Cloud Logging

### Messages stuck in queue
- Check ack deadline (default: 600 seconds = 10 minutes)
- Investigate worker errors
- May need to purge and resubmit

## ğŸ“š API Reference

### Job Status Values

- `queued`: In Pub/Sub queue, waiting for worker
- `processing`: Worker is scraping
- `completed`: Success, result available
- `failed`: Error occurred
- `retrying`: Failed but will retry

### Progress Stages

- `starting`: Worker picked up job
- `profile`: Scraping company profile
- `website`: Analyzing website
- `financial`: Fetching financial data
- `news`: Gathering news
- `enrichment`: AI processing
- `saving`: Storing results
- `completed`: Done

## ğŸ‰ Benefits Summary

âœ… **10-100x faster** API responses
âœ… **Unlimited** concurrent scrapes
âœ… **Guaranteed** job completion
âœ… **Auto-retry** on failures
âœ… **Real-time** progress tracking
âœ… **Webhook** notifications
âœ… **Horizontal** scaling
âœ… **Cost-effective** (pay per use)

---

**Ready to scale!** ğŸš€
